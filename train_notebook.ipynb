{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport os\nimport re\nimport json\nimport csv\nimport string\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.data import Dataset\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom tqdm import tqdm\nfrom random import randrange\n\nMODEL_DIR = \"/kaggle/input/huggingface-bert/\"","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:25.316227Z","iopub.execute_input":"2021-06-07T13:59:25.316667Z","iopub.status.idle":"2021-06-07T13:59:25.323677Z","shell.execute_reply.started":"2021-06-07T13:59:25.31663Z","shell.execute_reply":"2021-06-07T13:59:25.322298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertWordPieceTokenizer(\"../input/scibert-210605/vocab.txt\", lowercase=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:25.32589Z","iopub.execute_input":"2021-06-07T13:59:25.326326Z","iopub.status.idle":"2021-06-07T13:59:25.385298Z","shell.execute_reply.started":"2021-06-07T13:59:25.32628Z","shell.execute_reply":"2021-06-07T13:59:25.383914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_json(path, pub_id):\n    filepath = path + \"{}.json\".format(pub_id)\n    with open(filepath, \"r\") as file:\n        return json.load(file)\n    raise Error(\"could not open json file at '{}'\".format(filepath))\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))\n\ndef concat_sections(sections):\n    return \" \".join(section['text'] for section in sections)\n\ndef find_matches(text, label):\n    esc_label = re.escape(label) # TODO ignore case\n    return [match.start() for match in re.finditer(esc_label, text)]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:25.387998Z","iopub.execute_input":"2021-06-07T13:59:25.38843Z","iopub.status.idle":"2021-06-07T13:59:25.396917Z","shell.execute_reply.started":"2021-06-07T13:59:25.388384Z","shell.execute_reply":"2021-06-07T13:59:25.395419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_token_idx(offsets, start_char, end_char):\n    for start_idx, (offset_start, offset_end) in enumerate(offsets):\n        if offset_start <= start_char <= offset_end:\n            for end_idx, (offset_start, offset_end) in enumerate(offsets[start_idx:]):\n                if offset_start <= end_char <= offset_end:\n                    return start_idx, start_idx + end_idx\n    raise Exception('invalid token indices: ({}, {}) for offsets {}'.format(start_char, end_char, offsets))\n\ndef find_token_indices(tokenized, matches, match_len):\n    match_idx = 0\n    token_matches = []\n    for start_idx, (offset_start, offset_end) in enumerate(tokenized.offsets):\n        if offset_start <= matches[match_idx] <= offset_end:\n            end_char = matches[match_idx] + match_len\n            for end_idx, (offset_start, offset_end) in enumerate(tokenized.offsets[start_idx:]):\n                if offset_start <= end_char <= offset_end:\n                    token_matches.append((start_idx, start_idx + end_idx))\n                    \n                    match_idx += 1\n                    if match_idx >= len(matches):\n                        return token_matches\n                    break\n    raise Exception('could not find all matches in tokens')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:25.398762Z","iopub.execute_input":"2021-06-07T13:59:25.399213Z","iopub.status.idle":"2021-06-07T13:59:25.412218Z","shell.execute_reply.started":"2021-06-07T13:59:25.399166Z","shell.execute_reply":"2021-06-07T13:59:25.410839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunk_text(text):\n    tokenized = tokenizer.encode(text)\n    tokens = tokenized.ids\n\n    chunk_count = int(math.ceil(len(tokens) / CONTEXT_LEN))\n    \n    flattened_ids = np.zeros((chunk_count * CONTEXT_LEN,), dtype=np.float32)\n    flattened_masks = np.ones((chunk_count * CONTEXT_LEN,), dtype=np.float32)\n    \n    flattened_ids[:len(tokens)] = tokens\n    flattened_masks[len(tokens):] = 0\n\n    ids = flattened_ids.reshape((chunk_count, CONTEXT_LEN))\n    masks = flattened_masks.reshape((chunk_count, CONTEXT_LEN))\n    \n    return [ids, masks], tokenized.offsets[:-1] + [(len(text), len(text))]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:25.413594Z","iopub.execute_input":"2021-06-07T13:59:25.413888Z","iopub.status.idle":"2021-06-07T13:59:25.429935Z","shell.execute_reply.started":"2021-06-07T13:59:25.413854Z","shell.execute_reply":"2021-06-07T13:59:25.428739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONTEXT_LEN = 512\n\ndef make_example(tokenized, label_start, label_end):\n    label_len = label_end - label_start\n    half = (CONTEXT_LEN - label_len) // 2\n    shift = randrange(-half + 1, half)\n\n    left = label_start - (CONTEXT_LEN - label_len) // 2 - (label_len & 1 == 1) + shift\n    bounded_left = max(0, left)\n    left_padding = bounded_left - left\n\n    right = label_end + (CONTEXT_LEN - label_len) // 2 + shift\n    bounded_right = min(len(tokenized.ids), right)\n    right_padding = right - bounded_right\n\n    type_ids = [0] * left_padding + tokenized.ids[bounded_left:bounded_right] + [0] * right_padding\n    mask = [0] * left_padding + [1] * (CONTEXT_LEN - right_padding - left_padding) + [0] * right_padding\n    \n    left_margin = left_padding - bounded_left\n    start_idx = label_start + left_margin\n    end_idx = label_end + left_margin\n    \n    return ((type_ids, mask), (start_idx, end_idx))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:25.431239Z","iopub.execute_input":"2021-06-07T13:59:25.431554Z","iopub.status.idle":"2021-06-07T13:59:25.44269Z","shell.execute_reply.started":"2021-06-07T13:59:25.431501Z","shell.execute_reply":"2021-06-07T13:59:25.441643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_ids = []\nx_train_masks = []\ny_train_starts = []\ny_train_ends = []\ny_train_haslabel = []\n\nlabels = set()\n\nskip_count = 0\nwith open('../input/coleridgeinitiative-show-us-the-data/train.csv', 'r') as traincsv:\n    reader = csv.reader(traincsv)\n    next(reader) # skip headers\n    \n    for pub_id, _, _, db_label, _ in tqdm(reader):\n        labels.add(clean_text(db_label))\n        \n        sections = load_json(\"../input/coleridgeinitiative-show-us-the-data/train/\", pub_id)\n        text = concat_sections(sections)\n        (chunk_ids, chunk_masks), offsets = chunk_text(text)\n        \n        for start_idx, ids, mask in zip(range(0, len(offsets), CONTEXT_LEN), chunk_ids, chunk_masks):\n            start_offset = offsets[start_idx][0]\n            end_offset = offsets[min(start_idx + CONTEXT_LEN, len(offsets)) - 1][1] # NOTE: not sure if this works\n            orig_text = text[start_offset:end_offset]\n            matches = find_matches(orig_text, db_label)\n            local_offsets = offsets[start_idx:min(start_idx + CONTEXT_LEN, len(offsets))]\n            \n            if matches:\n                try:\n                    match_start = matches[0] + local_offsets[0][0]\n                    start, end = find_token_idx(local_offsets, match_start, match_start + len(db_label))\n\n                    x_train_ids.append(ids)\n                    x_train_masks.append(mask)\n                    y_train_starts.append(start)\n                    y_train_ends.append(end)\n                    y_train_haslabel.append(1.0)\n                except:\n                    skip_count += 1\n            else:\n                try:\n                    random_start = randrange(0, max(1, len(orig_text) - len(db_label))) + local_offsets[0][0]\n                    random_end = min(random_start + len(db_label), len(orig_text) + local_offsets[0][0])\n                    start, end = find_token_idx(local_offsets, random_start, random_end)\n\n                    x_train_ids.append(ids)\n                    x_train_masks.append(mask)\n                    y_train_starts.append(start)\n                    y_train_ends.append(end)\n                    y_train_haslabel.append(0.0)\n                except:\n                    skip_count += 1\n\nx_train = [np.array(x_train_ids, dtype=np.float32), np.array(x_train_masks, dtype=np.float32)]\ny_train = [np.array(y_train_starts, dtype=np.float32), np.array(y_train_ends, dtype=np.float32), np.array(y_train_haslabel, dtype=np.float32)]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T14:00:06.612438Z","iopub.execute_input":"2021-06-07T14:00:06.612802Z","iopub.status.idle":"2021-06-07T14:22:53.818561Z","shell.execute_reply.started":"2021-06-07T14:00:06.612771Z","shell.execute_reply":"2021-06-07T14:22:53.817211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_dataset(filepath, x_train, y_train):\n    np.savez_compressed(filepath, x_train_ids=x_train[0], x_train_masks=x_train[1], y_train_starts=y_train[0], y_train_ends=y_train[1], y_train_haslabel=y_train[2])\n\ndef load_dataset(filepath):\n    data = np.load(filepath)\n    return [data['x_train_ids'], data['x_train_masks']], [data['y_train_starts'], data['y_train_ends'], data['y_train_haslabel']]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T14:22:53.837989Z","iopub.execute_input":"2021-06-07T14:22:53.838427Z","iopub.status.idle":"2021-06-07T14:22:53.849464Z","shell.execute_reply.started":"2021-06-07T14:22:53.838393Z","shell.execute_reply":"2021-06-07T14:22:53.848405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dataset(\"unbalanced_dataset\", x_train, y_train)\n\nprint(len(y_train_haslabel))\nprint(sum(y_train_haslabel))\nprint(sum(1 - haslabel for haslabel in y_train_haslabel))\nprint(skip_count)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T14:25:15.166642Z","iopub.execute_input":"2021-06-07T14:25:15.167073Z","iopub.status.idle":"2021-06-07T14:25:15.206221Z","shell.execute_reply.started":"2021-06-07T14:25:15.167027Z","shell.execute_reply":"2021-06-07T14:25:15.205301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, y_train = load_dataset(\"./dataset.npz\")\npos_example_matches = y_train[2] >= 0.5\nneg_example_matches = y_train[2] < 0.5\nexamples_per_class = min(np.sum(pos_example_matches), np.sum(neg_example_matches))\nindices = np.arange(0, y_train[2].shape[0])\npos_indices = np.random.choice(indices[pos_example_matches], examples_per_class, replace=False)\nneg_indices = np.random.choice(indices[neg_example_matches], examples_per_class, replace=False)\ndata_indices = np.concatenate([pos_indices, neg_indices])\nnp.random.shuffle(data_indices)\n\nx_train_ids = x_train[0][data_indices]\nx_train_masks = x_train[1][data_indices]\ny_train_starts = y_train[0][data_indices]\ny_train_ends = y_train[1][data_indices]\ny_train_haslabels = y_train[2][data_indices]\nx_train = [x_train_ids, x_train_masks]\ny_train = [y_train_starts, y_train_ends, y_train_haslabels]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T14:56:26.775434Z","iopub.execute_input":"2021-06-07T14:56:26.775881Z","iopub.status.idle":"2021-06-07T14:56:35.203344Z","shell.execute_reply.started":"2021-06-07T14:56:26.775843Z","shell.execute_reply":"2021-06-07T14:56:35.202392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dataset(\"balanced_dataset\", x_train, y_train)\n\nprint(len(y_train[2]))\nprint(sum(y_train[2]))\nprint(sum(1 - haslabel for haslabel in y_train[2]))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T14:58:37.121262Z","iopub.execute_input":"2021-06-07T14:58:37.121708Z","iopub.status.idle":"2021-06-07T14:58:37.486611Z","shell.execute_reply.started":"2021-06-07T14:58:37.121671Z","shell.execute_reply":"2021-06-07T14:58:37.485529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\"../input/scibert-210605/\", from_pt=True)\n\n    input_ids = layers.Input(shape=(CONTEXT_LEN,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(CONTEXT_LEN,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n    \n    confidence = layers.Flatten()(embedding)\n    confidence = layers.Dense(1, name=\"confidence\", use_bias=False)(confidence)\n    confidence = layers.Activation(keras.activations.sigmoid)(confidence)\n\n    model = keras.Model(\n        inputs=[input_ids, attention_mask],\n        outputs=[start_probs, end_probs, confidence]\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss, keras.losses.BinaryCrossentropy(from_logits=False)])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:03:39.976106Z","iopub.execute_input":"2021-06-07T15:03:39.976592Z","iopub.status.idle":"2021-06-07T15:03:39.989126Z","shell.execute_reply.started":"2021-06-07T15:03:39.976495Z","shell.execute_reply":"2021-06-07T15:03:39.987897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:30.319788Z","iopub.status.idle":"2021-06-07T13:59:30.32017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(\n    x_train,\n    y_train,\n    epochs=3,\n    verbose=2,\n    batch_size=64,\n    callbacks=[],\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:30.320851Z","iopub.status.idle":"2021-06-07T13:59:30.321229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_excerpts(pub_id):\n    sections = load_json(\"../input/coleridgeinitiative-show-us-the-data/test/\", pub_id)\n    text = concat_sections(sections)\n    (chunk_ids, chunk_masks), offsets = chunk_text(text)\n    \n    start_probs, end_probs, confs = model.predict([chunk_ids, chunk_masks])\n    \n    label_chunks = np.ravel(confs) >= 0.5\n    starts, ends = np.argmax(start_probs[label_chunks], axis=1), np.argmax(end_probs[label_chunks], axis=1) + 1\n    return [tokenizer.decode(chunk[start:end + 1].astype(int)) for (chunk, start, end) in zip(chunk_ids[label_chunks], starts, ends)]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:30.321915Z","iopub.status.idle":"2021-06-07T13:59:30.322299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"../input/coleridgeinitiative-show-us-the-data/test\"\n_, _, filenames = next(os.walk(test_path))\n\nwith open(\"submission.csv\", \"w\") as submissions:\n    writer = csv.writer(submissions)\n\n    # headers\n    writer.writerow([\"Id\", \"PredictionString\"])\n    # entries\n    for filename in filenames:\n        pub_id = filename[:-5]\n        excerpts = make_excerpts(pub_id)\n        writer.writerow([pub_id, \"|\".join(set(clean_text(excerpt) for excerpt in excerpts))])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:30.323444Z","iopub.status.idle":"2021-06-07T13:59:30.323862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open(\"model_weights.dat\", 'wb') as file:\n    weights = [layer.get_weights() for layer in model.layers]\n    pickle.dump(weights, file)\n\nwith open(\"db_labels.dat\", 'wb') as file:\n    pickle.dump(labels, file)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T13:59:30.324787Z","iopub.status.idle":"2021-06-07T13:59:30.325187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}