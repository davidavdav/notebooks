{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Inspired by\nhttps://github.com/aitorzip/PyTorch-CycleGAN"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nmonet_files = []\nphoto_files = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n    if dirname == \"/kaggle/input/gan-getting-started/monet_jpg\":\n        for filename in filenames:\n            monet_files.append(os.path.join(dirname, filename))\n    elif dirname == \"/kaggle/input/gan-getting-started/photo_jpg\":\n        for filename in filenames:\n            photo_files.append(os.path.join(dirname, filename))\n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nprint(len(monet_files))\nprint(len(photo_files))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torch.autograd import Variable\n\nimport itertools\nfrom PIL import Image\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset Def\n\nNote, that the dataset has an imbalance, hence we are using only 1/7 of the training pictures and loop over the monet pictures.\n\nImprove this in the future\n- Add transforms to artificially create more monet images (random crop, horizontal flip etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GANDataset(Dataset):\n    def __init__(self,photo_list,monet_list,transform_monet,transform_photo):\n        #self.photo_list = photo_list\n        #self.monet_list = monet_list\n        self.transform_monet = transform_monet\n        self.transform_photo = transform_photo\n        self.data = {'Photo': photo_list, 'Monet': monet_list}\n        self.len_p = len(photo_list)\n        self.len_m = len(monet_list)\n        \n    def __len__(self):\n        #return int(np.ceil(self.len_p/7)) # use only the first 7th of the data\n        return self.len_p\n        #return self.len_p / \n    \n    def __getitem__(self, idx):\n        # Due to the imbalence (len_p=7028, len_m=300) we still can use all the photos if we loop through the monet pictures\n        imgP_path = os.path.join(self.data['Photo'][idx])\n        imgM_path = os.path.join(self.data['Monet'][idx%self.len_m])\n        imgP = Image.open(imgP_path)\n        imgM = Image.open(imgM_path)\n        return {'Photo': self.transform_photo(imgP), 'Monet': self.transform_monet(imgM)}\n    \nclass ImgAugment(object):\n    \n    def __init__(self,mean,std):\n        self.data_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean,std),\n            ])\n        self.data_augment = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean,std),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            #transforms.Resize([400,400]),\n            #transforms.RandomCrop(256),\n            #transforms.Resize([400,400]),\n            #transforms.RandomRotation(degrees=90),\n            #transforms.CenterCrop(256),\n            #transforms.GaussianBlur(11, sigma=(0.1, 2.0))\n            ])\n    def __call__(self,img):\n        rand = random.random()\n        if(rand < 0.2):\n            return self.data_augment(img)\n        else:\n            return self.data_transform(img)   \n    \n    # you ran: random flips + resize + crop + gaussian blur DONE\n    # todo: resize + crop + gaussian blur THIS RUN\n    # todo: resize + crop DONE\n    # todo: gaussian blur DONE\n\nclass ImgAugmentRandChoice(object):\n    def __init__(self,mean,std):\n        self.data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std),\n            transforms.RandomChoice([transforms.RandomHorizontalFlip(p=0.5),\n                                    transforms.RandomVerticalFlip(p=0.5),\n                                    transforms.GaussianBlur(5),\n                                    transforms.Compose([\n                                        transforms.Resize([400,400]),\n                                        transforms.RandomRotation(degrees=90),\n                                        transforms.CenterCrop(256)])\n                                    ]),\n                                    transforms.Compose([\n                                        transforms.Resize([400,400]),\n                                        transforms.RandomCrop(256)\n                                    ])\n            ])\n    def __call__(self,img):\n        rand = random.random()\n        if(rand < 0.2):\n            return self.data_augment(img)\n        else:\n            return self.data_transform(img)   \n    \nclass ImgTransform(object):\n    def __init__(self,mean,std):\n        self.data_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean,std)\n        ])\n    def __call__(self,img):\n        return self.data_transform(img)\n    \n#reference: https://www.kaggle.com/adrianda/cyclegan-pytorch-style-transfer\ndef reverse_normalize(image, mean_=0.5, std_=0.5):\n    if torch.is_tensor(image):\n        image = image.cpu().detach()\n    un_normalized_img = image * std_ + mean_\n    un_normalized_img = un_normalized_img * 255\n    #return un_normalized_img\n    return np.uint8(un_normalized_img)\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant(m.bias.data, 0.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = GANDataset(\n    photo_list=photo_files, monet_list=monet_files, \n    transform_monet = ImgAugment(mean=0.5,std=0.5), \n    transform_photo = ImgTransform(mean=0.5, std=0.5),\n)\nbatch_size = 1\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, \n                                               batch_size=batch_size, shuffle=True)\ndl_iter = iter(train_dataloader)\n\ndata = next(dl_iter)\np = data['Photo']\nm = data['Monet']\n\np = reverse_normalize(torch.squeeze(p)).transpose(1,2,0)\nm = reverse_normalize(torch.squeeze(m)).transpose(1,2,0)\n\nprint(p.shape)\n\nplt.imshow(p)\nplt.show()\nplt.imshow(m)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define models\n\nhttps://github.com/aitorzip/PyTorch-CycleGAN/blob/master/models.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [  nn.ReflectionPad2d(1),\n                        nn.Conv2d(in_features, in_features, 3),\n                        nn.InstanceNorm2d(in_features),\n                        nn.ReLU(inplace=True),\n                        nn.ReflectionPad2d(1),\n                        nn.Conv2d(in_features, in_features, 3),\n                        nn.InstanceNorm2d(in_features)  ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\nclass Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n        super(Generator, self).__init__()\n\n        # Initial convolution block       \n        model = [   nn.ReflectionPad2d(3),\n                    nn.Conv2d(input_nc, 64, 7),\n                    nn.InstanceNorm2d(64),\n                    nn.ReLU(inplace=True) ]\n\n        # Downsampling\n        in_features = 64\n        out_features = in_features*2\n        for _ in range(2):\n            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features*2\n\n        # Residual blocks\n        for _ in range(n_residual_blocks):\n            model += [ResidualBlock(in_features)]\n\n        # Upsampling\n        out_features = in_features//2\n        for _ in range(2):\n            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n                        nn.InstanceNorm2d(out_features),\n                        nn.ReLU(inplace=True) ]\n            in_features = out_features\n            out_features = in_features//2\n\n        # Output layer\n        model += [  nn.ReflectionPad2d(3),\n                    nn.Conv2d(64, output_nc, 7),\n                    nn.Tanh() ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_nc):\n        super(Discriminator, self).__init__()\n\n        # A bunch of convolutions one after another\n        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(128), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n                    nn.InstanceNorm2d(256), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        model += [  nn.Conv2d(256, 512, 4, padding=1),\n                    nn.InstanceNorm2d(512), \n                    nn.LeakyReLU(0.2, inplace=True) ]\n\n        # FCN classification layer\n        model += [nn.Conv2d(512, 1, 4, padding=1)]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        x =  self.model(x)\n        # Average pooling and flatten\n        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Init\n\n- Maybe include a learningrate that changes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = GANDataset(\n    photo_list=photo_files, monet_list=monet_files, \n    transform_monet = ImgAugment(mean=0.5,std=0.5), \n    #transform_monet = ImgAugmentRandChoice(mean=0.5,std=0.5),\n    transform_photo = ImgTransform(mean=0.5, std=0.5),\n)\n\n\nbatch_size = 1\nepochs = 6\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, \n                                               batch_size=batch_size, shuffle=True)\n\n# Define accelerator\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nnetG_P2M = Generator(3,3).to(device)\nnetG_M2P = Generator(3,3).to(device)\nnetD_P = Discriminator(3).to(device)\nnetD_M = Discriminator(3).to(device)\n\nnetG_P2M.apply(weights_init_normal)\nnetG_M2P.apply(weights_init_normal)\nnetD_P.apply(weights_init_normal)\nnetD_M.apply(weights_init_normal)\n\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_cycle = torch.nn.L1Loss()\ncriterion_identity = torch.nn.L1Loss()\n\noptimizer_G = torch.optim.Adam(itertools.chain(netG_P2M.parameters(), netG_M2P.parameters()),\n                                lr=2e-4, betas=(0.5, 0.999))\noptimizer_D_P = torch.optim.Adam(netD_P.parameters(), lr=2e-4, betas=(0.5, 0.999))\noptimizer_D_M = torch.optim.Adam(netD_M.parameters(), lr=2e-4, betas=(0.5, 0.999))\n\n# Inputs & targets memory allocation\nTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\ninput_P = Tensor(batch_size, 3, 256, 256)\ninput_M = Tensor(batch_size, 3, 256, 256)\ntarget_real = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)\ntarget_fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"losses_G = []\nlosses_DP = []\nlosses_DM = []\nlosses_IP = []\nlosses_IM = []\n\nbest_loss = 999.9\n\nfor epoch in range(epochs):\n    loss_G_sum = 0\n    loss_DP_sum = 0\n    loss_DM_sum = 0\n    loss_IP_sum = 0\n    loss_IM_sum = 0\n    \n    n_iterations = 0\n    \n    for i,batch in enumerate(train_dataloader):\n        # Set model input\n        real_P = Variable(input_P.copy_(batch['Photo']))\n        real_M = Variable(input_M.copy_(batch['Monet']))\n        \n         ###### Generators A2B and B2A ######\n        optimizer_G.zero_grad()\n\n        # Identity loss\n        # G_P2M(M) should equal M if real M is fed\n        same_M = netG_P2M(real_M)\n        loss_identity_M = criterion_identity(same_M, real_M)*5.0\n        # G_M2P(P) should equal P if real P is fed\n        same_P = netG_M2P(real_P)\n        loss_identity_P = criterion_identity(same_P, real_P)*5.0\n        \n        # GAN loss\n        fake_P = netG_P2M(real_P)\n        pred_fake = netD_M(fake_P)\n        loss_GAN_P2M = criterion_GAN(pred_fake, target_real)\n\n        fake_M = netG_M2P(real_M)\n        pred_fake = netD_P(fake_M)\n        loss_GAN_M2P = criterion_GAN(pred_fake, target_real)\n        \n         # Cycle loss\n        recovered_P = netG_M2P(fake_M)\n        loss_cycle_PMP = criterion_cycle(recovered_P, real_P)*10.0\n\n        recovered_M = netG_P2M(fake_P)\n        loss_cycle_MPM = criterion_cycle(recovered_M, real_M)*10.0\n    \n        # Total loss\n        loss_G = loss_identity_P + loss_identity_M + loss_GAN_P2M + loss_GAN_M2P + loss_cycle_PMP + loss_cycle_MPM\n        loss_G.backward()\n        \n        optimizer_G.step()\n        \n        \n        ###### Discriminator P ######\n        optimizer_D_P.zero_grad()\n\n        # Real loss\n        pred_real = netD_P(real_P)\n        loss_D_real = criterion_GAN(pred_real, target_real)\n\n        # Fake loss\n        #fake_A = fake_A_buffer.push_and_pop(fake_A)\n        pred_fake = netD_P(fake_P.detach())\n        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n\n        # Total loss\n        loss_D_P = (loss_D_real + loss_D_fake)*0.5\n        loss_D_P.backward()\n\n        optimizer_D_P.step()\n        \n        ###### Discriminator M ######\n        optimizer_D_M.zero_grad()\n\n        # Real loss\n        pred_real = netD_M(real_M)\n        loss_D_real = criterion_GAN(pred_real, target_real)\n        \n        # Fake loss\n        #fake_B = fake_B_buffer.push_and_pop(fake_B)\n        pred_fake = netD_M(fake_M.detach())\n        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n\n        # Total loss\n        loss_D_M = (loss_D_real + loss_D_fake)*0.5\n        loss_D_M.backward()\n\n        optimizer_D_M.step()\n        \n        loss_G_sum += loss_G.cpu().detach().numpy()\n        loss_DP_sum += loss_D_P.cpu().detach().numpy()\n        loss_DM_sum += loss_D_M.cpu().detach().numpy()\n        loss_IP_sum += loss_identity_P.cpu().detach().numpy()\n        loss_IM_sum += loss_identity_M.cpu().detach().numpy()\n        \n        n_iterations += 1\n        \n    if epoch > 10 and best_loss > loss_G:\n        torch.save(netG_P2M.state_dict(), './Photo2Monet_Gen.pt')\n        best_loss = loss_G\n        print(\"Model saved E: {}\".format(epoch))\n    losses_G.append(loss_G_sum/n_iterations)\n    losses_DP.append(loss_DP_sum/n_iterations)\n    losses_DM.append(loss_DM_sum/n_iterations)\n    losses_IP.append(loss_IP_sum/n_iterations)\n    losses_IM.append(loss_IM_sum/n_iterations)\n    \n    print(\"Epoch: {} lossGen: {} lossDP: {} lossDM: {}\".format(epoch,loss_G,loss_D_P,loss_D_M))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(losses_G)\nplt.title(\"Loss Generator\")\nplt.show()\nnp.save(\"./lossG.npy\",losses_G)\n\nplt.plot(losses_DP)\nplt.title(\"Loss Discriminator Photos\")\nplt.show()\nnp.save(\"./lossDP.npy\",losses_DP)\n\nplt.plot(losses_DM)\nplt.title(\"Loss Discriminator Monet\")\nplt.show()\nnp.save(\"./lossDM.npy\",losses_DM)\n\nplt.plot(losses_IP)\nplt.title(\"Loss Identity Photos\")\nplt.show()\nnp.save(\"./lossIP.npy\",losses_IP)\n\nplt.plot(losses_IM)\nplt.title(\"Loss Identity Monet\")\nplt.show()\nnp.save(\"./lossIM.npy\",losses_IM)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reverse"},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir ../images\n\n#load the best network\nfor file in os.listdir('./'):\n    if file.endswith('pt'):\n        print(file)\n        netG_P2M.load_state_dict(torch.load('./{0}/{1}'.format('./',file)))\n#\ntransform = ImgTransform(mean=0.5,std=0.5)\ni = 1\nfor img in photo_files:\n    imgP_trans = transform(Image.open(os.path.join(img)))\n    imgP_trans = imgP_trans[None,:,:,:]\n    imgP_trans = imgP_trans.to(device)\n    fakeM = netG_P2M(imgP_trans)\n    fakeM = torch.squeeze(fakeM)\n    fakeM = reverse_normalize(fakeM)\n    fakeM = fakeM.transpose(1,2,0) # bring color channel to last\n    if i < 10:\n        #plot 10 fake images\n        plt.subplot(121)\n        plt.imshow(reverse_normalize(torch.squeeze(imgP_trans)).transpose(1,2,0))\n        plt.subplot(122)\n        plt.imshow(fakeM)\n        plt.show()\n        \n    im = Image.fromarray(fakeM)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    \n    \n    i += 1\n    \n    \n    \nimport shutil\nshutil.make_archive(\"/kaggle/working/images\",'zip',\"/kaggle/images\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}