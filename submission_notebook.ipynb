{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport os\nimport re\nimport json\nimport csv\nimport string\nimport numpy as np\nimport tensorflow as tf\nimport pickle\nfrom tensorflow.data import Dataset\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom tqdm import tqdm\nfrom random import randrange\n\nCONTEXT_LEN = 512\nMODEL_DIR = \"/kaggle/input/huggingface-bert/\"\nconfiguration = BertConfig()  # default parameters and configuration for BERT\n\n# Load the fast tokenizer from saved file\ntokenizer = BertWordPieceTokenizer(\"../input/scibert-210605/vocab.txt\", lowercase=True)","metadata":{"_uuid":"850bc7e7-94c6-4cde-8736-bed3c7ee5b1f","_cell_guid":"f8039d12-9604-4fd7-abe7-4a9f99b0aac3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-08T13:19:35.333888Z","iopub.execute_input":"2021-06-08T13:19:35.334340Z","iopub.status.idle":"2021-06-08T13:19:35.365186Z","shell.execute_reply.started":"2021-06-08T13:19:35.334299Z","shell.execute_reply":"2021-06-08T13:19:35.364077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model():\n    ## BERT encoder\n    encoder = TFBertModel.from_pretrained(\"../input/scibert-210605/\", from_pt=True)\n\n    input_ids = layers.Input(shape=(CONTEXT_LEN,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(CONTEXT_LEN,), dtype=tf.int32)\n    embedding = encoder(\n        input_ids, attention_mask=attention_mask\n    )[0]\n\n    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n    start_logits = layers.Flatten()(start_logits)\n\n    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n    end_logits = layers.Flatten()(end_logits)\n\n    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n    \n    confidence = layers.Flatten()(embedding)\n    confidence = layers.Dense(1, name=\"confidence\", use_bias=False)(confidence)\n    confidence = layers.Activation(keras.activations.sigmoid)(confidence)\n\n    model = keras.Model(\n        inputs=[input_ids, attention_mask],\n        outputs=[start_probs, end_probs, confidence]\n    )\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n    optimizer = keras.optimizers.Adam(lr=5e-5)\n    model.compile(optimizer=optimizer, loss=[loss, loss, keras.losses.BinaryCrossentropy(from_logits=False)])\n    \n    with open(\"../input/coleridgebertweights/part2_scibert_len512.dat\", \"rb\") as file:\n        weights = pickle.load(file)\n        \n        for layer, layer_weights in zip(model.layers, weights):\n            if layer_weights:\n                layer.set_weights(layer_weights)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:19:35.367198Z","iopub.execute_input":"2021-06-08T13:19:35.367869Z","iopub.status.idle":"2021-06-08T13:19:35.379373Z","shell.execute_reply.started":"2021-06-08T13:19:35.367829Z","shell.execute_reply":"2021-06-08T13:19:35.378646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:19:35.382427Z","iopub.execute_input":"2021-06-08T13:19:35.382666Z","iopub.status.idle":"2021-06-08T13:19:38.030114Z","shell.execute_reply.started":"2021-06-08T13:19:35.382643Z","shell.execute_reply":"2021-06-08T13:19:38.029222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_json(path, pub_id):\n    filepath = path + \"{}.json\".format(pub_id)\n    with open(filepath, \"r\") as file:\n        return json.load(file)\n    raise Error(\"could not open json file at '{}'\".format(filepath))\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n\ndef concat_sections(sections):\n    return \" \".join(section['text'] for section in sections)\n\ndef find_matches(text, label):\n    esc_label = re.escape(label) # TODO ignore case\n    return [match.start() for match in re.finditer(esc_label, text)]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:19:38.031808Z","iopub.execute_input":"2021-06-08T13:19:38.032141Z","iopub.status.idle":"2021-06-08T13:19:38.039864Z","shell.execute_reply.started":"2021-06-08T13:19:38.032104Z","shell.execute_reply":"2021-06-08T13:19:38.038899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/crilabels/db_labels.dat\", \"rb\") as file:\n    labels = [clean_text(lbl) for lbl in pickle.load(file)]\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:19:38.040845Z","iopub.execute_input":"2021-06-08T13:19:38.041297Z","iopub.status.idle":"2021-06-08T13:19:38.056638Z","shell.execute_reply.started":"2021-06-08T13:19:38.041259Z","shell.execute_reply":"2021-06-08T13:19:38.055750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunk_text(text):\n    tokens = tokenizer.encode(text).ids\n\n    chunk_count = int(math.ceil(len(tokens) / CONTEXT_LEN))\n    \n    flattened_ids = np.zeros((chunk_count * CONTEXT_LEN,), dtype=np.float32)\n    flattened_masks = np.ones((chunk_count * CONTEXT_LEN,), dtype=np.float32)\n    \n    flattened_ids[:len(tokens)] = tokens\n    flattened_masks[len(tokens):] = 0\n    \n    ids = flattened_ids.reshape((chunk_count, CONTEXT_LEN))\n    masks = flattened_masks.reshape((chunk_count, CONTEXT_LEN))\n    \n    return [ids, masks]\n\ndef make_excerpts(text):\n    chunk_ids, chunk_masks = chunk_text(text)\n    start_probs, end_probs, confs = model.predict([chunk_ids, chunk_masks])\n    \n    label_chunks = np.ravel(confs) >= 0.5\n    starts, ends = np.argmax(start_probs[label_chunks], axis=1), np.argmax(end_probs[label_chunks], axis=1) + 1\n    excerpts = [tokenizer.decode(chunk[start:end + 1].astype(int)) for (chunk, start, end) in zip(chunk_ids[label_chunks], starts, ends)]\n    \n    cleaned_text = clean_text(text)\n    matched_labels = [lbl for lbl in labels if re.search(lbl, cleaned_text)]\n    excerpts.extend(matched_labels)\n    \n    return set(\" \".join(clean_text(excerpt).split()) for excerpt in excerpts)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:19:38.057834Z","iopub.execute_input":"2021-06-08T13:19:38.058454Z","iopub.status.idle":"2021-06-08T13:19:38.067994Z","shell.execute_reply.started":"2021-06-08T13:19:38.058419Z","shell.execute_reply":"2021-06-08T13:19:38.067114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"../input/coleridgeinitiative-show-us-the-data/test\"\n_, _, filenames = next(os.walk(test_path))\n\nwith open(\"submission.csv\", \"w\") as submissions:\n    writer = csv.writer(submissions)\n\n    # headers\n    writer.writerow([\"Id\", \"PredictionString\"])\n    # entries\n    for filename in filenames:\n        pub_id = filename[:-5]\n        sections = load_json(\"../input/coleridgeinitiative-show-us-the-data/test/\", pub_id)\n        text = concat_sections(sections)\n        excerpts = make_excerpts(text)\n        writer.writerow([pub_id, \"|\".join(excerpts)])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-08T13:19:38.069587Z","iopub.execute_input":"2021-06-08T13:19:38.070241Z","iopub.status.idle":"2021-06-08T13:19:43.649263Z","shell.execute_reply.started":"2021-06-08T13:19:38.070204Z","shell.execute_reply":"2021-06-08T13:19:43.648406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}